{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Original and Enhanced Pipelines in Intrusion Detection\n",
    "\n",
    "This notebook compares two pipelines for intrusion detection:\n",
    "\n",
    "- **Original Pipeline**: Data preprocessing and feature selection without Isolation Forest.\n",
    "- **Enhanced Pipeline**: Data preprocessing and feature selection with Isolation Forest as an anomaly detection filter.\n",
    "\n",
    "We aim to assess the impact of Isolation Forest on anomaly detection and dataset efficiency before applying tree-based models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries ----- ADDED SOME IMPORTS 4371 -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "\n",
    "# imports below are from 4371 group to make file work\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "# Core data manipulation and scientific libraries\n",
    "!pip install numpy pandas\n",
    "\n",
    "# Data visualization libraries\n",
    "!pip install seaborn matplotlib\n",
    "\n",
    "# Machine learning libraries\n",
    "!pip install scikit-learn xgboost\n",
    "\n",
    "# Imbalanced data handling\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "# Hyperparameter optimization libraries\n",
    "!pip install hyperopt scikit-optimize\n",
    "\n",
    "# (Optional) Additional dependencies for compatibility\n",
    "!pip install scipy\n",
    "\n",
    "# Custom module FCBF (if available locally, or if it's a GitHub repo, use the clone URL)\n",
    "# Replace \"URL_TO_FCBF_MODULE\" with the actual URL or location if it's on GitHub or a local file\n",
    "!pip install git+https://github.com/SantiagoEG/FCBF_module.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Isolation Forest Import -- 4371\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from FCBF_module import FCBF, FCBFK\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---Original Codebase Pipeline (Without Isolation Forest)---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the sampled CICIDS2017 dataset\n",
    "The CICIDS2017 dataset is publicly available at: https://www.unb.ca/cic/datasets/ids-2017.html  \n",
    "Due to the large size of this dataset, the sampled subsets of CICIDS2017 is used. The subsets are in the \"data\" folder.  \n",
    "If you want to use this code on other datasets (e.g., CAN-intrusion dataset), just change the dataset name and follow the same steps. The models in this code are generic models that can be used in any intrusion detection/network traffic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original Pipeline (Without Isolation Forest)\n",
    "print(\"---Original Pipeline (Without Isolation Forest)---\")\n",
    "\n",
    "# Read the sampled CICIDS2017 dataset\n",
    "df_orig = pd.read_csv('./data/CICIDS2017_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (normalization and padding values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "features_orig = df_orig.dtypes[df_orig.dtypes != 'object'].index\n",
    "df_orig[features_orig] = df_orig[features_orig].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std())\n",
    ")\n",
    "# Fill empty values by 0\n",
    "df_orig = df_orig.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "Due to the space limit of GitHub files and the large size of network traffic data, we sample a small-sized subset for model learning using **k-means cluster sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_orig = LabelEncoder()\n",
    "df_orig.iloc[:, -1] = labelencoder_orig.fit_transform(df_orig.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain the minority class instances and sample the majority class instances\n",
    "df_minor_orig = df_orig[\n",
    "    (df_orig['Label'] == 6) | (df_orig['Label'] == 1) | (df_orig['Label'] == 4)\n",
    "]\n",
    "df_major_orig = df_orig.drop(df_minor_orig.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = df_major_orig.drop(['Label'], axis=1)\n",
    "y_orig = df_major_orig['Label'].values\n",
    "y_orig=np.ravel(y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use k-means to cluster the data samples and select a proportion of data from each cluster\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans_orig = MiniBatchKMeans(n_clusters=1000, random_state=0).fit(X_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klabel_orig = kmeans_orig.labels_\n",
    "df_major_orig['klabel'] = klabel_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_major_orig['klabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_orig = list(df_major_orig)\n",
    "cols_orig.insert(78, cols_orig.pop(cols_orig.index('Label')))\n",
    "df_major_orig = df_major_orig.loc[:, cols_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typicalSampling_orig(group):\n",
    "    name = group.name\n",
    "    frac = 0.008\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "result_orig = df_major_orig.groupby(\n",
    "    'klabel', group_keys=False\n",
    ").apply(typicalSampling_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_orig['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4371 Had to modify the file below because the recommended way to combine DataFrames in recent versions of pandas is by using the pandas.concat() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'result' and 'df_minor' are already defined and loaded\n",
    "\n",
    "# No need to drop 'klabel' since it doesn't exist\n",
    "# If you need to drop another column, ensure it exists\n",
    "# For example, to drop 'Label' (only if intended, which is usually not the case):\n",
    "# result = result.drop(['Label'], axis=1)\n",
    "\n",
    "# Concatenate 'result_orig' and 'df_minor_orig' DataFrames\n",
    "result_orig = pd.concat([result_orig, df_minor_orig], ignore_index=True)\n",
    "\n",
    "print(\"DataFrames concatenated successfully.\")\n",
    "print(\"Updated DataFrame head:\")\n",
    "print(result_orig.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_orig.to_csv('./data/CICIDS2017_sample_km_orig.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Pipeline (Without Isolation Forest)\n",
    "print(\"---Original Pipeline (Without Isolation Forest)---\")\n",
    "\n",
    "# Read the sampled CICIDS2017 dataset\n",
    "df_orig = pd.read_csv('./data/CICIDS2017_sample_km_orig.csv')\n",
    "print(df_orig.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- ADDED LINES BELOW 4371 TO FIX ISSUE WITH ValueError: Input X contains NaN. ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object with the desired strategy (mean, median, most_frequent)\n",
    "imputer_orig = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the DataFrame\n",
    "df_orig[df_orig.columns] = imputer_orig.fit_transform(df_orig)\n",
    "\n",
    "# fixed the issue by using SimpleImputer to replace the NaN values in your dataset with \n",
    "# meaningful statistical estimates (like the mean of each feature column). This transformation eliminated missing \n",
    "# values from the dataset, which allowed mutual_info_classif to execute without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = df_orig.drop(['Label'], axis=1).values\n",
    "y_orig = df_orig['Label'].values\n",
    "y_orig=np.ravel(y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_orig, y_orig, train_size=0.8, test_size=0.2, random_state=0, stratify=y_orig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances_orig = mutual_info_classif(X_train_orig, y_train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of importance scores\n",
    "f_list_orig = sorted(\n",
    "    zip(map(lambda x: round(x, 4), importances_orig), features_orig), reverse=True\n",
    ")\n",
    "Sum_orig = sum([score for score, _ in f_list_orig])\n",
    "\n",
    "# Initialize Sum variable\n",
    "Sum = 0\n",
    "fs = []\n",
    "\n",
    "for i in range(0, len(f_list_orig)):\n",
    "    Sum = Sum + f_list_orig[i][0]\n",
    "    fs.append(f_list_orig[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the important features from top to bottom until the accumulated importance reaches 90%\n",
    "f_list2 = sorted(\n",
    "    zip(map(lambda x: round(x, 4), importances_orig / Sum_orig), features_orig),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "Sum2 = 0\n",
    "fs_selected = []\n",
    "\n",
    "for i in range(0, len(f_list2)):\n",
    "    Sum2 = Sum2 + f_list2[i][0]\n",
    "    fs_selected.append(f_list2[i][1])\n",
    "    if Sum2 >= 0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names from f_list_orig\n",
    "feature_names_orig = [name for score, name in f_list_orig]\n",
    "\n",
    "# Now use the list of feature names to select columns\n",
    "X_fs_orig = df_orig[feature_names_orig].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by Fast Correlation Based Filter (FCBF)\n",
    "\n",
    "The module is imported from the GitHub repo: https://github.com/SantiagoEG/FCBF_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
    "fcbf_orig = FCBFK(k=20)\n",
    "#fcbf.fit(X_fs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss_orig = fcbf_orig.fit_transform(X_fs_orig, y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-split train & test sets after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_fss_orig, y_orig, train_size=0.8, test_size=0.2, random_state=0, stratify=y_orig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data on origial codebase pipeline without isolation forest filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution in training data\n",
    "print(\"Original Training Data Class Distribution:\")\n",
    "print(pd.Series(y_train_orig).value_counts())\n",
    "\n",
    "# Dataset size\n",
    "print(f\"Original Training Data Shape: {X_train_orig.shape}\")\n",
    "print(f\"Original Test Data Shape: {X_test_orig.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "rf_orig = RandomForestClassifier(random_state=42)\n",
    "rf_orig.fit(X_train_orig, y_train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_orig = rf_orig.predict(X_test_orig)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report for Original Pipeline:\")\n",
    "print(classification_report(y_test_orig, y_pred_orig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix_orig = confusion_matrix(y_test_orig, y_pred_orig)\n",
    "print(\"Confusion Matrix for Original Pipeline:\")\n",
    "print(conf_matrix_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy and F1 score\n",
    "accuracy_orig = accuracy_score(y_test_orig, y_pred_orig)\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig, average='weighted')\n",
    "\n",
    "print(f\"Accuracy for Original Pipeline: {accuracy_orig:.4f}\")\n",
    "print(f\"Weighted F1 Score for Original Pipeline: {f1_orig:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---Modified Pipeline (With Isolation Forest)---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the sampled CICIDS2017 dataset\n",
    "The CICIDS2017 dataset is publicly available at: https://www.unb.ca/cic/datasets/ids-2017.html  \n",
    "Due to the large size of this dataset, the sampled subsets of CICIDS2017 is used. The subsets are in the \"data\" folder.  \n",
    "If you want to use this code on other datasets (e.g., CAN-intrusion dataset), just change the dataset name and follow the same steps. The models in this code are generic models that can be used in any intrusion detection/network traffic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enhanced Pipeline (With Isolation Forest)\n",
    "print(\"---Enhanced Pipeline (With Isolation Forest)---\")\n",
    "\n",
    "df_enh = pd.read_csv('./data/CICIDS2017_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enh.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (normalization and padding values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "features_enh = df_enh.dtypes[df_enh.dtypes != 'object'].index\n",
    "df_enh[features_enh] = df_enh[features_enh].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std())\n",
    ")\n",
    "df_enh = df_enh.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "Due to the space limit of GitHub files and the large size of network traffic data, we sample a small-sized subset for model learning using **k-means cluster sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_enh = LabelEncoder()\n",
    "df_enh.iloc[:, -1] = labelencoder_enh.fit_transform(df_enh.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enh.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain the minority class instances and sample the majority class instances\n",
    "df_minor_enh = df_enh[\n",
    "    (df_enh['Label'] == 6) | (df_enh['Label'] == 1) | (df_enh['Label'] == 4)\n",
    "]\n",
    "df_major_enh = df_enh.drop(df_minor_enh.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enh = df_major_enh.drop(['Label'], axis=1)\n",
    "y_enh = df_major_enh['Label'].values\n",
    "y_enh=np.ravel(y_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use k-means to cluster the data samples and select a proportion of data from each cluster\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans_enh = MiniBatchKMeans(n_clusters=1000, random_state=0).fit(X_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klabel_enh = kmeans_enh.labels_\n",
    "df_major_enh['klabel'] = klabel_enh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_major_enh['klabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging columns if necessary\n",
    "cols_enh = list(df_major_enh)\n",
    "cols_enh.insert(78, cols_enh.pop(cols_enh.index('Label')))\n",
    "df_major_enh = df_major_enh.loc[:, cols_enh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typicalSampling_enh(group):\n",
    "    name = group.name\n",
    "    frac = 0.008\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "result_enh = df_major_enh.groupby(\n",
    "    'klabel', group_keys=False\n",
    ").apply(typicalSampling_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_enh['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4371 Had to modify the file below because the recommended way to combine DataFrames in recent versions of pandas is by using the pandas.concat() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'result' and 'df_minor' are already defined and loaded\n",
    "\n",
    "# No need to drop 'klabel' since it doesn't exist\n",
    "# If you need to drop another column, ensure it exists\n",
    "# For example, to drop 'Label' (only if intended, which is usually not the case):\n",
    "# result = result.drop(['Label'], axis=1)\n",
    "\n",
    "# Concatenate 'result' and 'df_minor' DataFrames\n",
    "result_enh = pd.concat([result_enh, df_minor_enh], ignore_index=True)\n",
    "\n",
    "print(\"DataFrames concatenated successfully.\")\n",
    "print(\"Updated DataFrame head:\")\n",
    "print(result_enh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_enh.to_csv('./data/CICIDS2017_sample_km_enh.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Pipeline (With Isolation Forest)\n",
    "print(\"---Enhanced Pipeline (With Isolation Forest)---\")\n",
    "\n",
    "# Read the sampled CICIDS2017 dataset\n",
    "df_enh = pd.read_csv('./data/CICIDS2017_sample_km.csv')\n",
    "print(df_enh.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- ADDED LINES BELOW 4371 TO FIX ISSUE WITH ValueError: Input X contains NaN. ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object with the desired strategy (mean, median, most_frequent)\n",
    "imputer_enh = SimpleImputer(strategy='mean')\n",
    "df_enh[df_enh.columns] = imputer_enh.fit_transform(df_enh)\n",
    "\n",
    "# fixed the issue by using SimpleImputer to replace the NaN values in your dataset with \n",
    "# meaningful statistical estimates (like the mean of each feature column). This transformation eliminated missing \n",
    "# values from the dataset, which allowed mutual_info_classif to execute without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enh = df_enh.drop(['Label'], axis=1).values\n",
    "y_enh = df_enh['Label'].values\n",
    "y_enh=np.ravel(y_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_enh, y_enh, train_size=0.8, test_size=0.2, random_state=0, stratify=y_enh\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances_enh = mutual_info_classif(X_enh, y_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features_enh if not already defined\n",
    "features_enh = df_enh.dtypes[df_enh.dtypes != 'object'].index\n",
    "\n",
    "# calculate the sum of importance scores\n",
    "f_list_enh = sorted(zip(map(lambda x: round(x, 4), importances_enh), features_enh), reverse=True)\n",
    "Sum_enh = 0\n",
    "fs_enh = []\n",
    "for i in range(0, len(f_list_enh)):\n",
    "    Sum_enh = Sum_enh + f_list_enh[i][0]\n",
    "    fs_enh.append(f_list_enh[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the important features from top to bottom until the accumulated importance reaches 90%\n",
    "f_list2_enh = sorted(zip(map(lambda x: round(x, 4), importances_enh / Sum_enh), features_enh), reverse=True)\n",
    "Sum2_enh = 0\n",
    "fs_enh = []\n",
    "for i in range(0, len(f_list2_enh)):\n",
    "    Sum2_enh = Sum2_enh + f_list2_enh[i][0]\n",
    "    fs_enh.append(f_list2_enh[i][1])\n",
    "    if Sum2_enh >= 0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs_enh = df_enh[fs_enh].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs_enh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by Fast Correlation Based Filter (FCBF)\n",
    "\n",
    "The module is imported from the GitHub repo: https://github.com/SantiagoEG/FCBF_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
    "fcbf_enh = FCBFK(k=20)\n",
    "#fcbf.fit(X_fs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss_enh = fcbf_enh.fit_transform(X_fs_enh, y_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss_enh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest Implementation \n",
    "\n",
    "After performing feature selection using Information Gain (IG) and Fast Correlation-Based Filter (FCBF), we apply the Isolation Forest to detect and filter out anomalies in our dataset. This step enhances our model's ability to differentiate between actual threats and benign unusual behavior by removing potential outliers before training.\n",
    "\n",
    "n_estimators=100: Number of trees in the forest.\n",
    "contamination='auto': Let the algorithm decide the proportion of anomalies.\n",
    "random_state=42: For reproducibility.\n",
    "Anomaly Detection:\n",
    "\n",
    "anomaly_predictions == 1: Inliers (normal instances).\n",
    "anomaly_predictions == -1: Outliers (anomalies).\n",
    "Filtering Data:\n",
    "\n",
    "X_filtered: Contains only the inlier instances.\n",
    "y_filtered: Corresponding labels for inliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isolation Forest\n",
    "iso_forest_enh = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "iso_forest_enh.fit(X_fss_enh)\n",
    "\n",
    "# Obtain anomaly scores and predictions\n",
    "anomaly_scores_enh = iso_forest_enh.decision_function(X_fss_enh)\n",
    "anomaly_predictions_enh = iso_forest_enh.predict(X_fss_enh)\n",
    "\n",
    "# Filter out anomalies\n",
    "inlier_mask_enh = anomaly_predictions_enh == 1\n",
    "X_filtered_enh = X_fss_enh[inlier_mask_enh]\n",
    "y_filtered_enh = y_enh[inlier_mask_enh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_filtered_enh is defined: {'y_filtered_enh' in locals()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify indices of removed anomalies\n",
    "removed_mask_enh = anomaly_predictions_enh == -1\n",
    "removed_anomalies_indices = np.where(removed_mask_enh)[0]\n",
    "\n",
    "# Since X_fss_enh is derived from df_enh, ensure indices align\n",
    "# If necessary, reset index of df_enh\n",
    "df_enh_reset = df_enh.reset_index(drop=True)\n",
    "\n",
    "# Get the labels of the removed anomalies\n",
    "removed_anomalies = df_enh_reset.iloc[removed_anomalies_indices]\n",
    "removed_labels = removed_anomalies['Label']\n",
    "\n",
    "print(\"Labels of Removed Anomalies:\")\n",
    "print(removed_labels.value_counts())\n",
    "\n",
    "# Calculate the proportion of each class in removed anomalies\n",
    "removed_label_counts = removed_labels.value_counts()\n",
    "total_removed = removed_label_counts.sum()\n",
    "removed_label_proportions = (removed_label_counts / total_removed) * 100\n",
    "\n",
    "print(\"\\nProportion of Each Class in Removed Anomalies:\")\n",
    "print(removed_label_proportions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Anomaly Scores\n",
    "\n",
    "To understand how the Isolation Forest has assigned anomaly scores to our data points, we visualize the distribution of these scores. This helps us assess the threshold and proportion of data considered anomalous, providing insights into the filtering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=removed_label_counts.index, y=removed_label_counts.values, palette='viridis')\n",
    "plt.title('Removed Anomalies by Class')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Removed Samples')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels of retained inliers\n",
    "retained_labels = y_filtered_enh\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=retained_labels, palette='pastel')\n",
    "plt.title('Retained Inliers Class Distribution')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=removed_labels, palette='magma')\n",
    "plt.title('Removed Anomalies Class Distribution')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define benign and threat classes\n",
    "benign_classes = [0.0]  # Assuming 0.0 corresponds to 'BENIGN'\n",
    "threat_classes = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]  # Other classes are threats\n",
    "\n",
    "# Calculate counts in removed anomalies\n",
    "benign_removed = removed_labels[removed_labels.isin(benign_classes)].count()\n",
    "threat_removed = removed_labels[removed_labels.isin(threat_classes)].count()\n",
    "\n",
    "print(f\"Benign Anomalies Removed: {benign_removed}\")\n",
    "print(f\"Threat Anomalies Removed: {threat_removed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Out Detected Anomalies\n",
    "\n",
    "Using the predictions from the Isolation Forest, we filter out the anomalies (outliers) from our dataset. We retain only the inlier data points (those predicted as normal) for model training. This step aims to improve the quality of our training data by removing noise and potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out anomalies\n",
    "inlier_mask_enh = anomaly_predictions_enh == 1\n",
    "X_filtered_enh = X_fss_enh[inlier_mask_enh]\n",
    "y_filtered_enh = y_enh[inlier_mask_enh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-split train & test sets after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split after filtering\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_filtered_enh, y_filtered_enh, train_size=0.8, test_size=0.2, random_state=0, stratify=y_filtered_enh\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train_enh).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shapes and class distributions\n",
    "print(f\"Enhanced Training Data Shape: {X_train_enh.shape}\")\n",
    "print(f\"Enhanced Test Data Shape: {X_test_enh.shape}\")\n",
    "print(\"Enhanced Training Data Class Distribution:\")\n",
    "print(pd.Series(y_train_enh).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "rf_enh = RandomForestClassifier(random_state=42)\n",
    "rf_enh.fit(X_train_enh, y_train_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_enh = rf_enh.predict(X_test_enh)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report for Enhanced Pipeline:\")\n",
    "print(classification_report(y_test_enh, y_pred_enh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix_enh = confusion_matrix(y_test_enh, y_pred_enh)\n",
    "print(\"Confusion Matrix for Enhanced Pipeline:\")\n",
    "print(conf_matrix_enh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy and F1 score\n",
    "accuracy_enh = accuracy_score(y_test_enh, y_pred_enh)\n",
    "f1_enh = f1_score(y_test_enh, y_pred_enh, average='weighted')\n",
    "\n",
    "print(f\"Accuracy for Enhanced Pipeline: {accuracy_enh:.4f}\")\n",
    "print(f\"Weighted F1 Score for Enhanced Pipeline: {f1_enh:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Comparison of Original pipeline against Isolation Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples_before = X_fss_enh.shape[0]\n",
    "total_samples_after = X_filtered_enh.shape[0]\n",
    "anomalies_removed = total_samples_before - total_samples_after\n",
    "anomaly_percentage = (anomalies_removed / total_samples_before) * 100\n",
    "\n",
    "print(f\"Total Samples Before Isolation Forest: {total_samples_before}\")\n",
    "print(f\"Total Samples After Isolation Forest: {total_samples_after}\")\n",
    "print(f\"Anomalies Detected and Removed: {anomalies_removed}\")\n",
    "print(f\"Percentage of Anomalies Detected: {anomaly_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare overall metrics\n",
    "print(\"Comparison of Pipelines:\")\n",
    "print(f\"Accuracy - Original Pipeline: {accuracy_orig:.4f}\")\n",
    "print(f\"Accuracy - Enhanced Pipeline: {accuracy_enh:.4f}\\n\")\n",
    "\n",
    "print(f\"Weighted F1 Score - Original Pipeline: {f1_orig:.4f}\")\n",
    "print(f\"Weighted F1 Score - Enhanced Pipeline: {f1_enh:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Original Pipeline\n",
    "disp_orig = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_orig, display_labels=rf_orig.classes_)\n",
    "disp_orig.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix - Original Pipeline')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for Enhanced Pipeline\n",
    "disp_enh = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_enh, display_labels=rf_enh.classes_)\n",
    "disp_enh.plot(cmap='Greens')\n",
    "plt.title('Confusion Matrix - Enhanced Pipeline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports as dictionaries\n",
    "report_orig = classification_report(y_test_orig, y_pred_orig, output_dict=True)\n",
    "report_enh = classification_report(y_test_enh, y_pred_enh, output_dict=True)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_report_orig = pd.DataFrame(report_orig).transpose()\n",
    "df_report_enh = pd.DataFrame(report_enh).transpose()\n",
    "\n",
    "# Per-class F1-scores\n",
    "print(\"Per-class F1-scores for Original Pipeline:\")\n",
    "print(df_report_orig['f1-score'])\n",
    "\n",
    "print(\"\\nPer-class F1-scores for Enhanced Pipeline:\")\n",
    "print(df_report_enh['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to compare\n",
    "classes = df_report_orig.index[:-3]  # Exclude 'accuracy', 'macro avg', 'weighted avg'\n",
    "\n",
    "# Extract F1 scores\n",
    "f1_scores_orig = df_report_orig.loc[classes, 'f1-score']\n",
    "f1_scores_enh = df_report_enh.loc[classes, 'f1-score']\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, f1_scores_orig, width, label='Original Pipeline')\n",
    "rects2 = ax.bar(x + width/2, f1_scores_enh, width, label='Enhanced Pipeline')\n",
    "\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Per-Class F1 Score Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(classes, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Distribution Before Isolation Forest:\")\n",
    "print(pd.Series(y_enh).value_counts())\n",
    "\n",
    "print(\"\\nClass Distribution After Isolation Forest:\")\n",
    "print(pd.Series(y_filtered_enh).value_counts())\n",
    "\n",
    "# Plotting class distributions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before Isolation Forest\n",
    "sns.countplot(x=y_enh, ax=ax[0])\n",
    "ax[0].set_title('Before Isolation Forest')\n",
    "ax[0].set_xlabel('Class')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "# After Isolation Forest\n",
    "sns.countplot(x=y_filtered_enh, ax=ax[1])\n",
    "ax[1].set_title('After Isolation Forest')\n",
    "ax[1].set_xlabel('Class')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset sizes\n",
    "print(\"Original Training Data Shape:\", X_train_orig.shape)\n",
    "print(\"Original Test Data Shape:\", X_test_orig.shape)\n",
    "\n",
    "# Enhanced dataset sizes\n",
    "print(\"\\nEnhanced Training Data Shape:\", X_train_enh.shape)\n",
    "print(\"Enhanced Test Data Shape:\", X_test_enh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Samples',\n",
    "        'Anomalies Detected and Removed',\n",
    "        'Percentage of Anomalies Detected',\n",
    "        'Training Data Shape',\n",
    "        'Test Data Shape',\n",
    "        # 'Time Taken for Preprocessing (seconds)'  # If you have timing data\n",
    "    ],\n",
    "    'Original Pipeline': [\n",
    "        X_fss_orig.shape[0],\n",
    "        'N/A',\n",
    "        'N/A',\n",
    "        X_train_orig.shape,\n",
    "        X_test_orig.shape,\n",
    "        # f\"{time_orig:.2f}\"\n",
    "    ],\n",
    "    'Enhanced Pipeline': [\n",
    "        X_filtered_enh.shape[0],\n",
    "        anomalies_removed,\n",
    "        f\"{anomaly_percentage:.2f}%\",\n",
    "        X_train_enh.shape,\n",
    "        X_test_enh.shape,\n",
    "        # f\"{time_enh:.2f}\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare anomaly scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(anomaly_scores_enh, shade=True, color='red')\n",
    "plt.title('Density Plot of Anomaly Scores')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "summary_data = {\n",
    "    'Metric': ['Accuracy', 'Weighted F1 Score'],\n",
    "    'Original Pipeline': [accuracy_orig, f1_orig],\n",
    "    'Enhanced Pipeline': [accuracy_enh, f1_enh]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Based on the evaluation metrics, the Enhanced Pipeline demonstrates improved performance over the Original Pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
