{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Original and Enhanced Pipelines in Intrusion Detection\n",
    "\n",
    "This notebook compares two pipelines for intrusion detection:\n",
    "\n",
    "- **Original Pipeline**: Data preprocessing and feature selection without Isolation Forest.\n",
    "- **Enhanced Pipeline**: Data preprocessing and feature selection with Isolation Forest as an anomaly detection filter.\n",
    "\n",
    "We aim to assess the impact of Isolation Forest on anomaly detection and dataset efficiency before applying tree-based models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries ----- ADDED SOME IMPORTS 4371 -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: hyperopt in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: scikit-optimize in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (1.13.1)\n",
      "Requirement already satisfied: six in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (3.2.1)\n",
      "Requirement already satisfied: future in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (4.66.6)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (3.1.0)\n",
      "Requirement already satisfied: py4j in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-optimize) (24.9.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-optimize) (1.5.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-optimize) (24.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\logan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Collecting git+https://github.com/SantiagoEG/FCBF_module.git\n",
      "  Cloning https://github.com/SantiagoEG/FCBF_module.git to c:\\users\\logan\\appdata\\local\\temp\\pip-req-build-8_hp_4rb\n",
      "  Resolved https://github.com/SantiagoEG/FCBF_module.git to commit 092b60b65ee6ceaf9b0227d12b575f2a3336b287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/SantiagoEG/FCBF_module.git 'C:\\Users\\Logan\\AppData\\Local\\Temp\\pip-req-build-8_hp_4rb'\n",
      "ERROR: git+https://github.com/SantiagoEG/FCBF_module.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "\n",
    "# imports below are from 4371 group to make file work\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "# Core data manipulation and scientific libraries\n",
    "!pip install numpy pandas\n",
    "\n",
    "# Data visualization libraries\n",
    "!pip install seaborn matplotlib\n",
    "\n",
    "# Machine learning libraries\n",
    "!pip install scikit-learn xgboost\n",
    "\n",
    "# Imbalanced data handling\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "# Hyperparameter optimization libraries\n",
    "!pip install hyperopt scikit-optimize\n",
    "\n",
    "# (Optional) Additional dependencies for compatibility\n",
    "!pip install scipy\n",
    "\n",
    "# Custom module FCBF (if available locally, or if it's a GitHub repo, use the clone URL)\n",
    "# Replace \"URL_TO_FCBF_MODULE\" with the actual URL or location if it's on GitHub or a local file\n",
    "!pip install git+https://github.com/SantiagoEG/FCBF_module.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Isolation Forest Import -- 4371\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---Original Codebase Pipeline (Without Isolation Forest)---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the sampled CICIDS2017 dataset\n",
    "The CICIDS2017 dataset is publicly available at: https://www.unb.ca/cic/datasets/ids-2017.html  \n",
    "Due to the large size of this dataset, the sampled subsets of CICIDS2017 is used. The subsets are in the \"data\" folder.  \n",
    "If you want to use this code on other datasets (e.g., CAN-intrusion dataset), just change the dataset name and follow the same steps. The models in this code are generic models that can be used in any intrusion detection/network traffic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Original Pipeline (Without Isolation Forest)---\n"
     ]
    }
   ],
   "source": [
    "# Original Pipeline (Without Isolation Forest)\n",
    "print(\"---Original Pipeline (Without Isolation Forest)---\")\n",
    "\n",
    "# Read the sampled CICIDS2017 dataset\n",
    "df_orig = pd.read_csv('./data/CICIDS2017_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "BENIGN          22731\n",
       "DoS             19035\n",
       "PortScan         7946\n",
       "BruteForce       2767\n",
       "WebAttack        2180\n",
       "Bot              1966\n",
       "Infiltration       36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (normalization and padding values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "features_orig = df_orig.dtypes[df_orig.dtypes != 'object'].index\n",
    "df_orig[features_orig] = df_orig[features_orig].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std())\n",
    ")\n",
    "# Fill empty values by 0\n",
    "df_orig = df_orig.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "Due to the space limit of GitHub files and the large size of network traffic data, we sample a small-sized subset for model learning using **k-means cluster sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_orig = LabelEncoder()\n",
    "df_orig.iloc[:, -1] = labelencoder_orig.fit_transform(df_orig.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    22731\n",
       "3    19035\n",
       "5     7946\n",
       "2     2767\n",
       "6     2180\n",
       "1     1966\n",
       "4       36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain the minority class instances and sample the majority class instances\n",
    "df_minor_orig = df_orig[\n",
    "    (df_orig['Label'] == 6) | (df_orig['Label'] == 1) | (df_orig['Label'] == 4)\n",
    "]\n",
    "df_major_orig = df_orig.drop(df_minor_orig.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = df_major_orig.drop(['Label'], axis=1)\n",
    "y_orig = df_major_orig['Label'].values\n",
    "y_orig=np.ravel(y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use k-means to cluster the data samples and select a proportion of data from each cluster\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans_orig = MiniBatchKMeans(n_clusters=1000, random_state=0).fit(X_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "klabel_orig = kmeans_orig.labels_\n",
    "df_major_orig['klabel'] = klabel_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "klabel\n",
       "20     482\n",
       "842    411\n",
       "312    348\n",
       "324    337\n",
       "745    334\n",
       "      ... \n",
       "973      1\n",
       "727      1\n",
       "594      1\n",
       "410      1\n",
       "100      1\n",
       "Name: count, Length: 979, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_major_orig['klabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_orig = list(df_major_orig)\n",
    "cols_orig.insert(78, cols_orig.pop(cols_orig.index('Label')))\n",
    "df_major_orig = df_major_orig.loc[:, cols_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typicalSampling_orig(group):\n",
    "    name = group.name\n",
    "    frac = 0.008\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "result_orig = df_major_orig.groupby(\n",
    "    'klabel', group_keys=False\n",
    ").apply(typicalSampling_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "3    120\n",
       "0    119\n",
       "5     57\n",
       "2     18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_orig['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4371 Had to modify the file below because the recommended way to combine DataFrames in recent versions of pandas is by using the pandas.concat() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames concatenated successfully.\n",
      "Updated DataFrame head:\n",
      "   Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
      "0       1.721079           0.037025                0.008402   \n",
      "1      -0.523395          -0.068426               -0.051737   \n",
      "2      -0.486085          -0.050851               -0.021667   \n",
      "3      -0.368223          -0.015701               -0.081806   \n",
      "4      -0.512273          -0.015701               -0.081806   \n",
      "\n",
      "   Total Length of Fwd Packets  Total Length of Bwd Packets  \\\n",
      "0                    -0.031683                     0.057881   \n",
      "1                    -0.030559                    -0.046494   \n",
      "2                    -0.033088                     0.057881   \n",
      "3                    -0.032901                    -0.048343   \n",
      "4                    -0.032901                    -0.048343   \n",
      "\n",
      "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
      "0              -0.218767              -0.211174               -0.207684   \n",
      "1              -0.188874               0.624667               -0.018124   \n",
      "2              -0.218767              -0.211174               -0.198110   \n",
      "3              -0.239691              -0.085798               -0.213428   \n",
      "4              -0.239691              -0.085798               -0.213428   \n",
      "\n",
      "   Fwd Packet Length Std  Bwd Packet Length Max  ...  Active Mean  Active Std  \\\n",
      "0              -0.229934               1.545614  ...     0.445788   -0.081786   \n",
      "1              -0.255104              -0.523309  ...    -0.109889   -0.081786   \n",
      "2              -0.209439               3.620304  ...    -0.109889   -0.081786   \n",
      "3              -0.255104              -0.559719  ...    -0.108537   -0.081786   \n",
      "4              -0.255104              -0.559719  ...    -0.109889   -0.081786   \n",
      "\n",
      "   Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  klabel  \\\n",
      "0    0.309175    0.506398   2.061361 -0.137651  1.987010  2.096251     2.0   \n",
      "1   -0.128620   -0.094860  -0.458729 -0.137651 -0.466440 -0.442057     8.0   \n",
      "2   -0.128620   -0.094860  -0.458729 -0.137651 -0.466440 -0.442057    10.0   \n",
      "3   -0.127555   -0.093397  -0.275668 -0.137651 -0.288221 -0.257674    11.0   \n",
      "4   -0.128620   -0.094860  -0.458729 -0.137651 -0.466440 -0.442057    12.0   \n",
      "\n",
      "   Label  \n",
      "0      3  \n",
      "1      0  \n",
      "2      3  \n",
      "3      3  \n",
      "4      3  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'result' and 'df_minor' are already defined and loaded\n",
    "\n",
    "# No need to drop 'klabel' since it doesn't exist\n",
    "# If you need to drop another column, ensure it exists\n",
    "# For example, to drop 'Label' (only if intended, which is usually not the case):\n",
    "# result = result.drop(['Label'], axis=1)\n",
    "\n",
    "# Concatenate 'result_orig' and 'df_minor_orig' DataFrames\n",
    "result_orig = pd.concat([result_orig, df_minor_orig], ignore_index=True)\n",
    "\n",
    "print(\"DataFrames concatenated successfully.\")\n",
    "print(\"Updated DataFrame head:\")\n",
    "print(result_orig.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_orig.to_csv('./data/CICIDS2017_sample_km_orig.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Original Pipeline (Without Isolation Forest)---\n",
      "Flow Duration                     0\n",
      "Total Fwd Packets                 0\n",
      "Total Backward Packets            0\n",
      "Total Length of Fwd Packets       0\n",
      "Total Length of Bwd Packets       0\n",
      "                               ... \n",
      "Idle Std                          0\n",
      "Idle Max                          0\n",
      "Idle Min                          0\n",
      "klabel                         4182\n",
      "Label                             0\n",
      "Length: 79, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Original Pipeline (Without Isolation Forest)\n",
    "print(\"---Original Pipeline (Without Isolation Forest)---\")\n",
    "\n",
    "# Read the sampled CICIDS2017 dataset\n",
    "df_orig = pd.read_csv('./data/CICIDS2017_sample_km_orig.csv')\n",
    "print(df_orig.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- ADDED LINES BELOW 4371 TO FIX ISSUE WITH ValueError: Input X contains NaN. ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object with the desired strategy (mean, median, most_frequent)\n",
    "imputer_orig = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the DataFrame\n",
    "df_orig[df_orig.columns] = imputer_orig.fit_transform(df_orig)\n",
    "\n",
    "# fixed the issue by using SimpleImputer to replace the NaN values in your dataset with \n",
    "# meaningful statistical estimates (like the mean of each feature column). This transformation eliminated missing \n",
    "# values from the dataset, which allowed mutual_info_classif to execute without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = df_orig.drop(['Label'], axis=1).values\n",
    "y_orig = df_orig['Label'].values\n",
    "y_orig=np.ravel(y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_orig, y_orig, train_size=0.8, test_size=0.2, random_state=0, stratify=y_orig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances_orig = mutual_info_classif(X_train_orig, y_train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (4146424994.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    fs.append(f_list_orig[i][1\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum of importance scores\n",
    "f_list_orig = sorted(\n",
    "    zip(map(lambda x: round(x, 4), importances_orig), features_orig), reverse=True\n",
    ")\n",
    "Sum_orig = sum([score for score, _ in f_list_orig])\n",
    "\n",
    "# Initialize Sum variable\n",
    "Sum = 0\n",
    "fs = []\n",
    "\n",
    "for i in range(0, len(f_list_orig)):\n",
    "    Sum = Sum + f_list_orig[i][0]\n",
    "    fs.append(f_list_orig[i][1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the important features from top to bottom until the accumulated importance reaches 90%\n",
    "f_list2 = sorted(\n",
    "    zip(map(lambda x: round(x, 4), importances_orig / Sum_orig), features_orig),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "Sum2 = 0\n",
    "fs_selected = []\n",
    "\n",
    "for i in range(0, len(f_list2)):\n",
    "    Sum2 = Sum2 + f_list2[i][0]\n",
    "    fs_selected.append(f_list2[i][1])\n",
    "    if Sum2 >= 0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_fs_orig \u001b[38;5;241m=\u001b[39m df_orig[\u001b[43mfs_orig\u001b[49m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fs_orig' is not defined"
     ]
    }
   ],
   "source": [
    "X_fs_orig = df_orig[fs_orig].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by Fast Correlation Based Filter (FCBF)\n",
    "\n",
    "The module is imported from the GitHub repo: https://github.com/SantiagoEG/FCBF_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
    "fcbf_orig = FCBFK(k=20)\n",
    "#fcbf.fit(X_fs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss_orig = fcbf_orig.fit_transform(X_fs_orig, y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-split train & test sets after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_fss_orig, y_orig, train_size=0.8, test_size=0.2, random_state=0, stratify=y_orig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train_orig).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data on origial codebase pipeline without isolation forest filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution in training data\n",
    "print(\"Original Training Data Class Distribution:\")\n",
    "print(pd.Series(y_train_orig).value_counts())\n",
    "\n",
    "# Dataset size\n",
    "print(f\"Original Training Data Shape: {X_train_orig.shape}\")\n",
    "print(f\"Original Test Data Shape: {X_test_orig.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---Modified Pipeline (With Isolation Forest)---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the sampled CICIDS2017 dataset\n",
    "The CICIDS2017 dataset is publicly available at: https://www.unb.ca/cic/datasets/ids-2017.html  \n",
    "Due to the large size of this dataset, the sampled subsets of CICIDS2017 is used. The subsets are in the \"data\" folder.  \n",
    "If you want to use this code on other datasets (e.g., CAN-intrusion dataset), just change the dataset name and follow the same steps. The models in this code are generic models that can be used in any intrusion detection/network traffic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "df = pd.read_csv('./data/CICIDS2017_sample.csv') \n",
    "# The results in this code is based on the original CICIDS2017 dataset. Please go to cell [21] if you work on the sampled dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (normalization and padding values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "features = df.dtypes[df.dtypes != 'object'].index\n",
    "df[features] = df[features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# Fill empty values by 0\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "Due to the space limit of GitHub files and the large size of network traffic data, we sample a small-sized subset for model learning using **k-means cluster sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df.iloc[:, -1] = labelencoder.fit_transform(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain the minority class instances and sample the majority class instances\n",
    "df_minor = df[(df['Label']==6)|(df['Label']==1)|(df['Label']==4)]\n",
    "df_major = df.drop(df_minor.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_major.drop(['Label'],axis=1) \n",
    "y = df_major.iloc[:, -1].values.reshape(-1,1)\n",
    "y=np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use k-means to cluster the data samples and select a proportion of data from each cluster\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=1000, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klabel=kmeans.labels_\n",
    "df_major['klabel']=klabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_major['klabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_major)\n",
    "cols.insert(78, cols.pop(cols.index('Label')))\n",
    "df_major = df_major.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typicalSampling(group):\n",
    "    name = group.name\n",
    "    frac = 0.008\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "result = df_major.groupby(\n",
    "    'klabel', group_keys=False\n",
    ").apply(typicalSampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4371 Had to modify the file below because the recommended way to combine DataFrames in recent versions of pandas is by using the pandas.concat() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'result' and 'df_minor' are already defined and loaded\n",
    "\n",
    "# No need to drop 'klabel' since it doesn't exist\n",
    "# If you need to drop another column, ensure it exists\n",
    "# For example, to drop 'Label' (only if intended, which is usually not the case):\n",
    "# result = result.drop(['Label'], axis=1)\n",
    "\n",
    "# Concatenate 'result' and 'df_minor' DataFrames\n",
    "result = pd.concat([result, df_minor], ignore_index=True)\n",
    "\n",
    "print(\"DataFrames concatenated successfully.\")\n",
    "print(\"Updated DataFrame head:\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./data/CICIDS2017_sample_km.csv',index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Pipeline (With Isolation Forest)\n",
    "print(\"---Enhanced Pipeline (With Isolation Forest)---\")\n",
    "\n",
    "# Read the sampled CICIDS2017 dataset\n",
    "df_enh = pd.read_csv('./data/CICIDS2017_sample_km.csv')\n",
    "print(df_enh.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- ADDED LINES BELOW 4371 TO FIX ISSUE WITH ValueError: Input X contains NaN. ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object with the desired strategy (mean, median, most_frequent)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the DataFrame\n",
    "df[df.columns] = imputer.fit_transform(df)\n",
    "\n",
    "# fixed the issue by using SimpleImputer to replace the NaN values in your dataset with \n",
    "# meaningful statistical estimates (like the mean of each feature column). This transformation eliminated missing \n",
    "# values from the dataset, which allowed mutual_info_classif to execute without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Label'],axis=1).values\n",
    "y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "y=np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, random_state = 0,stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances = mutual_info_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum of importance scores\n",
    "f_list = sorted(zip(map(lambda x: round(x, 4), importances), features), reverse=True)\n",
    "Sum = 0\n",
    "fs = []\n",
    "for i in range(0, len(f_list)):\n",
    "    Sum = Sum + f_list[i][0]\n",
    "    fs.append(f_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the important features from top to bottom until the accumulated importance reaches 90%\n",
    "f_list2 = sorted(zip(map(lambda x: round(x, 4), importances/Sum), features), reverse=True)\n",
    "Sum2 = 0\n",
    "fs = []\n",
    "for i in range(0, len(f_list2)):\n",
    "    Sum2 = Sum2 + f_list2[i][0]\n",
    "    fs.append(f_list2[i][1])\n",
    "    if Sum2>=0.9:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs = df[fs].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection by Fast Correlation Based Filter (FCBF)\n",
    "\n",
    "The module is imported from the GitHub repo: https://github.com/SantiagoEG/FCBF_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
    "fcbf = FCBFK(k = 20)\n",
    "#fcbf.fit(X_fs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss = fcbf.fit_transform(X_fs,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest Implementation \n",
    "\n",
    "After performing feature selection using Information Gain (IG) and Fast Correlation-Based Filter (FCBF), we apply the Isolation Forest to detect and filter out anomalies in our dataset. This step enhances our model's ability to differentiate between actual threats and benign unusual behavior by removing potential outliers before training.\n",
    "\n",
    "n_estimators=100: Number of trees in the forest.\n",
    "contamination='auto': Let the algorithm decide the proportion of anomalies.\n",
    "random_state=42: For reproducibility.\n",
    "Anomaly Detection:\n",
    "\n",
    "anomaly_predictions == 1: Inliers (normal instances).\n",
    "anomaly_predictions == -1: Outliers (anomalies).\n",
    "Filtering Data:\n",
    "\n",
    "X_filtered: Contains only the inlier instances.\n",
    "y_filtered: Corresponding labels for inliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "iso_forest.fit(X_fss)\n",
    "\n",
    "# Obtain anomaly scores and predictions\n",
    "anomaly_scores = iso_forest.decision_function(X_fss)\n",
    "anomaly_predictions = iso_forest.predict(X_fss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Anomaly Scores\n",
    "\n",
    "To understand how the Isolation Forest has assigned anomaly scores to our data points, we visualize the distribution of these scores. This helps us assess the threshold and proportion of data considered anomalous, providing insights into the filtering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(anomaly_scores, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Anomaly Scores')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Out Detected Anomalies\n",
    "\n",
    "Using the predictions from the Isolation Forest, we filter out the anomalies (outliers) from our dataset. We retain only the inlier data points (those predicted as normal) for model training. This step aims to improve the quality of our training data by removing noise and potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out anomalies\n",
    "inlier_mask = anomaly_predictions == 1\n",
    "X_filtered = X_fss[inlier_mask]\n",
    "y_filtered = y[inlier_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-split train & test sets after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split after filtering\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_filtered, y_filtered, train_size=0.8, test_size=0.2, random_state=0, stratify=y_filtered\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution in training data\n",
    "print(\"Enhanced Training Data Class Distribution:\")\n",
    "print(pd.Series(y_train_enh).value_counts())\n",
    "\n",
    "# Dataset size\n",
    "print(f\"Enhanced Training Data Shape: {X_filtered.shape}\")\n",
    "print(f\"Enhanced Test Data Shape: {X_test_enh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Comparison of Original pipeline against Isolation Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of samples before and after anomaly detection\n",
    "total_samples_before = X_fss.shape[0]\n",
    "total_samples_after = X_filtered.shape[0]\n",
    "anomalies_removed = total_samples_before - total_samples_after\n",
    "\n",
    "print(f\"Total Samples Before Isolation Forest: {total_samples_before}\")\n",
    "print(f\"Total Samples After Isolation Forest: {total_samples_after}\")\n",
    "print(f\"Anomalies Detected and Removed: {anomalies_removed}\")\n",
    "\n",
    "# Percentage of anomalies detected\n",
    "anomaly_percentage = (anomalies_removed / total_samples_before) * 100\n",
    "print(f\"Percentage of Anomalies Detected: {anomaly_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution before Isolation Forest\n",
    "print(\"Class Distribution Before Isolation Forest:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "\n",
    "# Class distribution after Isolation Forest\n",
    "print(\"\\nClass Distribution After Isolation Forest:\")\n",
    "print(pd.Series(y_filtered).value_counts())\n",
    "\n",
    "# Plotting class distributions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before Isolation Forest\n",
    "sns.countplot(x=y, ax=ax[0])\n",
    "ax[0].set_title('Before Isolation Forest')\n",
    "ax[0].set_xlabel('Class')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "# After Isolation Forest\n",
    "sns.countplot(x=y_filtered, ax=ax[1])\n",
    "ax[1].set_title('After Isolation Forest')\n",
    "ax[1].set_xlabel('Class')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset sizes\n",
    "print(\"Original Training Data Shape:\", X_train_orig.shape)\n",
    "print(\"Original Test Data Shape:\", X_test_orig.shape)\n",
    "\n",
    "# Enhanced dataset sizes\n",
    "print(\"\\nEnhanced Training Data Shape:\", X_train_enh.shape)\n",
    "print(\"Enhanced Test Data Shape:\", X_test_enh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Samples',\n",
    "        'Anomalies Detected and Removed',\n",
    "        'Percentage of Anomalies Detected',\n",
    "        'Training Data Shape',\n",
    "        'Test Data Shape',\n",
    "        'Time Taken for Preprocessing (seconds)'\n",
    "    ],\n",
    "    'Original Pipeline': [\n",
    "        total_samples_before,  # X_fss.shape[0]\n",
    "        'N/A',\n",
    "        'N/A',\n",
    "        X_train_orig.shape,\n",
    "        X_test_orig.shape,\n",
    "        f\"{time_orig:.2f}\"\n",
    "    ],\n",
    "    'Enhanced Pipeline': [\n",
    "        total_samples_after,  # X_filtered.shape[0]\n",
    "        anomalies_removed,\n",
    "        f\"{anomaly_percentage:.2f}%\",\n",
    "        X_train_enh.shape,\n",
    "        X_test_enh.shape,\n",
    "        f\"{time_enh:.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare anomaly scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(anomaly_scores, shade=True, color='red')\n",
    "plt.title('Density Plot of Anomaly Scores')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
